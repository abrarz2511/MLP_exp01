{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abrarz2511/MLP_exp01/blob/main/Assignment01_Abrar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mathematical Formulation for IMDB Text Classification using an MLP\n",
        "# Instructor: Dr. Ankur Mali\n",
        "# University of South Florida (Spring 2025)\n",
        "\n",
        "This document describes the mathematical framework for processing IMDB text data using a character-level bag-of-characters representation, passing it through a multi-layer perceptron (MLP), and training the model via gradient descent. The evaluation metrics include loss, accuracy, precision, and recall.\n",
        "\n",
        "---\n",
        "The findings and Recording of the experiment are recorded in the notebook after the programs for the given models.\n",
        "\n",
        "Name: Abrar Zahin\n",
        "## 1. Tokenization and Input Representation\n",
        "\n",
        "Given a raw text review \\( T \\), we first tokenize it at the character level. Let \\( V \\) be the vocabulary (i.e., the set of unique characters) extracted from the training data with size \\( |V| = d \\).\n",
        "\n",
        "For each text review \\( T \\), we construct a binary bag-of-characters vector \\( x \\in \\{0,1\\}^d \\) such that:\n",
        "\n",
        "$$\n",
        "x_j =\n",
        "\\begin{cases}\n",
        "1, & \\text{if the } j\\text{-th character in } V \\text{ appears in } T, \\\\\n",
        "0, & \\text{otherwise.}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Thus, each review is represented as:\n",
        "\n",
        "$$\n",
        "x = \\mathrm{BOW}(T) \\in \\mathbb{R}^d.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 2. MLP Model\n",
        "\n",
        "The MLP we consider has the following structure:\n",
        "- **Input layer:** Receives $$( x \\in \\mathbb{R}^d )$$.\n",
        "- **Hidden Layer 1:** With $h_1$ or $z_1$ (Post-activation) neurons.\n",
        "- **Hidden Layer 2:** With \\( h_2 \\) neurons.\n",
        "- **Output Layer:** With \\( c \\) neurons (for \\( c = 2 \\) classes in binary classification).\n",
        "\n",
        "### 2.1. Model Parameters\n",
        "\n",
        "- **First Hidden Layer:**\n",
        "  - Weight matrix: $$(W^{(1)} \\in \\mathbb{R}^{d \\times h_1} )$$\n",
        "  - Bias vector: $$( b^{(1)} \\in \\mathbb{R}^{h_1} )$$\n",
        "\n",
        "- **Second Hidden Layer:**\n",
        "  - Weight matrix: $$( W^{(2)} \\in \\mathbb{R}^{h_1 \\times h_2} )$$\n",
        "  - Bias vector: $$( b^{(2)} \\in \\mathbb{R}^{h_2} )$$\n",
        "\n",
        "- **Output Layer:**\n",
        "  - Weight matrix: $$( W^{(3)} \\in \\mathbb{R}^{h_2 \\times c} )$$\n",
        "  - Bias vector: $$( b^{(3)} \\in \\mathbb{R}^{c} )$$\n",
        "\n",
        "> **Note:** In the original code, a third hidden layer size (\\( h_3 \\)) is provided as a parameter but is not used in the forward computation. Here, the model uses two hidden layers. You can add any N layers, to this pipeline, remember to modify the pipeline accordingly.\n",
        "\n",
        "### 2.2. Forward Pass\n",
        "\n",
        "For an input vector \\( x \\), the forward propagation through the network is as follows:\n",
        "\n",
        "1. **First Hidden Layer:**\n",
        "\n",
        "   $$\n",
        "   h^{(1)} = \\text{ReLU}\\Big( x\\, W^{(1)} + b^{(1)} \\Big)\n",
        "   $$\n",
        "\n",
        "2. **Second Hidden Layer:**\n",
        "\n",
        "   $$\n",
        "   h^{(2)} = \\text{ReLU}\\Big( h^{(1)}\\, W^{(2)} + b^{(2)} \\Big)\n",
        "   $$\n",
        "\n",
        "3. **Output Layer (Logits):**\n",
        "\n",
        "   $$\n",
        "   z = h^{(2)}\\, W^{(3)} + b^{(3)}\n",
        "   $$\n",
        "\n",
        "The logits \\( z \\) are then converted to class probabilities using the softmax function:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\text{softmax}(z) = \\frac{\\exp(z)}{\\sum_{j=1}^{c} \\exp(z_j)}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Loss Function\n",
        "\n",
        "We use the **Categorical Cross Entropy Loss** (with logits) for training. For a single sample with true one-hot label \\( y \\) and predicted probabilities \\( \\hat{y} \\), the loss is:\n",
        "\n",
        "$$\n",
        "L(y, \\hat{y}) = -\\sum_{j=1}^{c} y_j \\log(\\hat{y}_j)\n",
        "$$\n",
        "\n",
        "For a batch of \\( N \\) samples, the average loss is computed as:\n",
        "\n",
        "$$\n",
        "L = \\frac{1}{N} \\sum_{i=1}^{N} L(y^{(i)}, \\hat{y}^{(i)})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Training via Gradient Descent\n",
        "\n",
        "The goal is to minimize the loss \\( \\mathcal{L} \\) with respect to the model parameters:\n",
        "\n",
        "$$\n",
        "\\Theta = \\{ W^{(1)},\\, b^{(1)},\\, W^{(2)},\\, b^{(2)},\\, W^{(3)},\\, b^{(3)} \\}\n",
        "$$\n",
        "\n",
        "Using gradient descent (or an adaptive method like Adam), each parameter \\( \\theta \\in \\Theta \\) is updated as:\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - \\eta\\, \\nabla_\\theta L\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\eta$ is the learning rate.\n",
        "- $\\nabla_\\theta L $ denotes the gradient of the loss with respect to $\\theta $.\n",
        "\n",
        "Backpropagation is used to compute these gradients efficiently.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Evaluation Metrics\n",
        "\n",
        "In addition to monitoring the loss during training, we evaluate the model performance using:\n",
        "\n",
        "- **Accuracy:**\n",
        "\n",
        "  $$\n",
        "  \\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\n",
        "  $$\n",
        "\n",
        "- **Precision:**\n",
        "\n",
        "  $$\n",
        "  \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
        "  $$\n",
        "\n",
        "- **Recall:**\n",
        "\n",
        "  $$\n",
        "  \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
        "  $$\n",
        "\n",
        "These metrics are computed on the validation and test sets to assess the model’s generalization performance.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Summary of the Pipeline\n",
        "\n",
        "1. **Tokenization:**  \n",
        "   Each review \\( T \\) is tokenized at the character level and converted into a binary vector $$x \\in \\{0,1\\}^d$$ representing the presence of each character in the vocabulary \\( V \\).\n",
        "\n",
        "2. **MLP Forward Propagation:**  \n",
        "   The input vector \\( x \\) is propagated through the MLP:\n",
        "   - First hidden layer: $$ h^{(1)} = \\text{ReLU}\\big( x\\, W^{(1)} + b^{(1)} \\big) $$\n",
        "   - Second hidden layer: $$ h^{(2)} = \\text{ReLU}\\big( h^{(1)}\\, W^{(2)} + b^{(2)} \\big) $$\n",
        "   - Output layer: $$ z = h^{(2)}\\, W^{(3)} + b^{(3)} $$\n",
        "   - Softmax conversion: $$ \\hat{y} = \\text{softmax}(z) $$\n",
        "\n",
        "3. **Loss Computation:**  \n",
        "   The categorical cross entropy loss L is computed using the true labels and the predicted probabilities.\n",
        "\n",
        "4. **Training:**  \n",
        "   The model parameters $\\Theta$ are updated using gradient descent (or Adam), where:\n",
        "\n",
        "   $$\n",
        "   \\theta \\leftarrow \\theta - \\eta\\, \\nabla_\\theta L\n",
        "   $$\n",
        "\n",
        "5. **Evaluation:**  \n",
        "   After training, the model is evaluated on the validation and test sets using the loss, accuracy, precision, and recall metrics.\n",
        "\n",
        "---\n",
        "\n",
        "This formulation captures the entire process—from transforming raw text into a numeric representation, through the forward and backward passes of an MLP, to the training and evaluation of the system. Shorter version of your slides :)\n"
      ],
      "metadata": {
        "id": "W_It9C2TJ05K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP on IMDB Dataset"
      ],
      "metadata": {
        "id": "fNGpqID8UZaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "tf.random.set_seed(1221)\n",
        "np.random.seed(1221)\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.relu(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.relu(h2)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z2, self.W3) + self.b3\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=1000):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=False, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 256\n",
        "size_hidden2 = 128\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qb0sCnSFsHJl",
        "outputId": "7c14f53c-7b1a-4a1c-e3c9-1aaa5a6a020f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 80169\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.4650 | Val Loss: 0.3785 | Accuracy: 0.8362 | Precision: 0.8482 | Recall: 0.8065\n",
            "Epoch 02 | Training Loss: 0.3107 | Val Loss: 0.3574 | Accuracy: 0.8458 | Precision: 0.8408 | Recall: 0.8412\n",
            "Epoch 03 | Training Loss: 0.2617 | Val Loss: 0.3603 | Accuracy: 0.8472 | Precision: 0.8309 | Recall: 0.8597\n",
            "Epoch 04 | Training Loss: 0.2135 | Val Loss: 0.3768 | Accuracy: 0.8432 | Precision: 0.8522 | Recall: 0.8185\n",
            "Epoch 05 | Training Loss: 0.1606 | Val Loss: 0.4060 | Accuracy: 0.8376 | Precision: 0.8209 | Recall: 0.8507\n",
            "Epoch 06 | Training Loss: 0.1043 | Val Loss: 0.4599 | Accuracy: 0.8340 | Precision: 0.8111 | Recall: 0.8573\n",
            "Epoch 07 | Training Loss: 0.0585 | Val Loss: 0.5233 | Accuracy: 0.8308 | Precision: 0.8244 | Recall: 0.8271\n",
            "Epoch 08 | Training Loss: 0.0277 | Val Loss: 0.6087 | Accuracy: 0.8300 | Precision: 0.8106 | Recall: 0.8474\n",
            "Epoch 09 | Training Loss: 0.0119 | Val Loss: 0.6868 | Accuracy: 0.8288 | Precision: 0.8221 | Recall: 0.8255\n",
            "Epoch 10 | Training Loss: 0.0057 | Val Loss: 0.7629 | Accuracy: 0.8282 | Precision: 0.8172 | Recall: 0.8317\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.7303 | Test Accuracy: 0.8352 | Test Precision: 0.8336 | Test Recall: 0.8374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random MLP on IMDB Dataset"
      ],
      "metadata": {
        "id": "Zpy6iEakUWAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "tf.random.set_seed(1221)\n",
        "np.random.seed(1221)\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP_rnd(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        #self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "        self.variables = [self.W3, self.b3]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.relu(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.relu(h2)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z2, self.W3) + self.b3\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=1000):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=False, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 256\n",
        "size_hidden2 = 128\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP_rnd(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0MseUvkuqd0",
        "outputId": "9e68ffaf-92b2-474b-8b6d-2e164875cea3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 80169\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.7183 | Val Loss: 0.6921 | Accuracy: 0.5556 | Precision: 0.5432 | Recall: 0.5243\n",
            "Epoch 02 | Training Loss: 0.6803 | Val Loss: 0.6694 | Accuracy: 0.5898 | Precision: 0.5791 | Recall: 0.5635\n",
            "Epoch 03 | Training Loss: 0.6599 | Val Loss: 0.6573 | Accuracy: 0.6062 | Precision: 0.5964 | Recall: 0.5804\n",
            "Epoch 04 | Training Loss: 0.6481 | Val Loss: 0.6507 | Accuracy: 0.6154 | Precision: 0.5993 | Recall: 0.6238\n",
            "Epoch 05 | Training Loss: 0.6414 | Val Loss: 0.6466 | Accuracy: 0.6206 | Precision: 0.6015 | Recall: 0.6440\n",
            "Epoch 06 | Training Loss: 0.6366 | Val Loss: 0.6455 | Accuracy: 0.6234 | Precision: 0.5971 | Recall: 0.6861\n",
            "Epoch 07 | Training Loss: 0.6333 | Val Loss: 0.6430 | Accuracy: 0.6276 | Precision: 0.6032 | Recall: 0.6778\n",
            "Epoch 08 | Training Loss: 0.6312 | Val Loss: 0.6406 | Accuracy: 0.6318 | Precision: 0.6157 | Recall: 0.6399\n",
            "Epoch 09 | Training Loss: 0.6296 | Val Loss: 0.6408 | Accuracy: 0.6326 | Precision: 0.6088 | Recall: 0.6774\n",
            "Epoch 10 | Training Loss: 0.6283 | Val Loss: 0.6398 | Accuracy: 0.6350 | Precision: 0.6133 | Recall: 0.6687\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.6285 | Test Accuracy: 0.6448 | Test Precision: 0.6382 | Test Recall: 0.6686\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP with feedback alignment on IMDB Dataset"
      ],
      "metadata": {
        "id": "p-wETEHOUGuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_FA(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (Note: Not used in compute_output in this example)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used)\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # Create fixed random feedback matrices for feedback alignment:\n",
        "        # B3: used to propagate the error from the output layer to the second hidden layer.\n",
        "        # It replaces the use of W3^T. Its shape is (size_output, size_hidden2).\n",
        "        self.B3 = tf.Variable(tf.random.normal([self.size_output, self.size_hidden2]), trainable=False)\n",
        "\n",
        "        # B2: used to propagate the error from the second hidden layer to the first hidden layer.\n",
        "        # Its shape is (size_hidden2, size_hidden1).\n",
        "        self.B2 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden1]), trainable=False)\n",
        "\n",
        "        # Define variables to be updated during training\n",
        "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred - Tensor of shape (batch_size, size_output)\n",
        "        y_true - Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass using feedback alignment.\n",
        "        Computes gradients manually using fixed random feedback matrices.\n",
        "        X_train: Input data (numpy array)\n",
        "        y_train: One-hot encoded labels (numpy array)\n",
        "        Returns: List of gradients corresponding to [dW1, dW2, dW3, db1, db2, db3]\n",
        "        \"\"\"\n",
        "        # Cast input to float32 tensor\n",
        "        X_tf = tf.cast(X_train, tf.float32)\n",
        "\n",
        "        # --- Forward Pass ---\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        a1 = tf.nn.relu(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(a1, self.W2) + self.b2\n",
        "        a2 = tf.nn.relu(h2)\n",
        "        # Output layer (logits)\n",
        "        logits = tf.matmul(a2, self.W3) + self.b3\n",
        "        # Softmax predictions\n",
        "        y_pred = tf.nn.softmax(logits)\n",
        "\n",
        "        # --- Compute Output Error ---\n",
        "        # For cross-entropy with softmax, the derivative is (y_pred - y_true)\n",
        "        delta3 = y_pred - tf.cast(y_train, tf.float32)  # shape: (batch, size_output)\n",
        "        batch_size = tf.cast(tf.shape(X_tf)[0], tf.float32)\n",
        "\n",
        "        # --- Gradients for Output Layer ---\n",
        "        dW3 = tf.matmul(tf.transpose(a2), delta3) / batch_size\n",
        "        db3 = tf.reduce_mean(delta3, axis=0, keepdims=True)\n",
        "\n",
        "        # --- Feedback Alignment for Second Hidden Layer ---\n",
        "        # Instead of delta2 = (delta3 dot W3^T) * ReLU'(h2), use a fixed random matrix B3.\n",
        "        relu_grad_h2 = tf.cast(h2 > 0, tf.float32)\n",
        "        # delta3 has shape (batch, size_output) and B3 has shape (size_output, size_hidden2)\n",
        "        delta2 = tf.matmul(delta3, self.B3) * relu_grad_h2  # shape: (batch, size_hidden2)\n",
        "\n",
        "        dW2 = tf.matmul(tf.transpose(a1), delta2) / batch_size\n",
        "        db2 = tf.reduce_mean(delta2, axis=0, keepdims=True)\n",
        "\n",
        "        # --- Feedback Alignment for First Hidden Layer ---\n",
        "        # Instead of delta1 = (delta2 dot W2^T) * ReLU'(h1), use a fixed random matrix B2.\n",
        "        relu_grad_h1 = tf.cast(h1 > 0, tf.float32)\n",
        "        # delta2 has shape (batch, size_hidden2) and B2 has shape (size_hidden2, size_hidden1)\n",
        "        delta1 = tf.matmul(delta2, self.B2) * relu_grad_h1  # shape: (batch, size_hidden1)\n",
        "\n",
        "        dW1 = tf.matmul(tf.transpose(X_tf), delta1) / batch_size\n",
        "        db1 = tf.reduce_mean(delta1, axis=0, keepdims=True)\n",
        "\n",
        "        return [dW1, dW2, dW3, db1, db2, db3]\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to obtain output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.relu(h1)\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.relu(h2)\n",
        "        output = tf.matmul(z2, self.W3) + self.b3\n",
        "        return output\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=1000):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=False, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 256\n",
        "size_hidden2 = 128\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP_FA(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mG5AfD2DTdNy",
        "outputId": "bb65a9f3-40df-4cef-b4bd-71db18df4e02"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 80169\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.6719 | Val Loss: 0.4389 | Accuracy: 0.8062 | Precision: 0.7748 | Recall: 0.8461\n",
            "Epoch 02 | Training Loss: 0.3739 | Val Loss: 0.3686 | Accuracy: 0.8458 | Precision: 0.8087 | Recall: 0.8932\n",
            "Epoch 03 | Training Loss: 0.3217 | Val Loss: 0.3526 | Accuracy: 0.8550 | Precision: 0.8509 | Recall: 0.8498\n",
            "Epoch 04 | Training Loss: 0.3025 | Val Loss: 0.3559 | Accuracy: 0.8518 | Precision: 0.8203 | Recall: 0.8890\n",
            "Epoch 05 | Training Loss: 0.2823 | Val Loss: 0.3541 | Accuracy: 0.8530 | Precision: 0.8300 | Recall: 0.8762\n",
            "Epoch 06 | Training Loss: 0.2607 | Val Loss: 0.3703 | Accuracy: 0.8486 | Precision: 0.8200 | Recall: 0.8812\n",
            "Epoch 07 | Training Loss: 0.2344 | Val Loss: 0.3827 | Accuracy: 0.8430 | Precision: 0.8618 | Recall: 0.8053\n",
            "Epoch 08 | Training Loss: 0.2043 | Val Loss: 0.4065 | Accuracy: 0.8384 | Precision: 0.8669 | Recall: 0.7875\n",
            "Epoch 09 | Training Loss: 0.1729 | Val Loss: 0.4257 | Accuracy: 0.8432 | Precision: 0.8251 | Recall: 0.8585\n",
            "Epoch 10 | Training Loss: 0.1388 | Val Loss: 0.4613 | Accuracy: 0.8412 | Precision: 0.8297 | Recall: 0.8461\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.4454 | Test Accuracy: 0.8421 | Test Precision: 0.8389 | Test Recall: 0.8469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CylG3cMfXGjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "646# Assignment 2 Todos\n",
        "\n",
        "## Overview\n",
        "- **Objective:**  \n",
        "  Modify your model’s text preprocessing by changing from character-level tokenization to word-level tokenization. Compare the performance of both tokenization methods. Additionally, perform hyper-parameter optimization by experimenting with various settings (learning rate, hidden layers, hidden sizes, batch sizes, optimizers, and activation functions) and report your findings.\n",
        "\n",
        "## 1. Initial Setup\n",
        "- [Done] **Set Random Seeds:**  \n",
        "  Ensure reproducibility by setting seeds for all random number generators (e.g., Python’s `random`, NumPy, TensorFlow/PyTorch).\n",
        "  \n",
        "- [Done] **Prepare the Environment:**  \n",
        "  - Create a new or update an existing Jupyter Notebook.\n",
        "  - Ensure that all necessary libraries (e.g., NumPy, pandas, TensorFlow/PyTorch, matplotlib, etc.) are installed.\n",
        "  \n",
        "- [Done] **Version Control:**  \n",
        "  Initialize a Git repository (if not already done) and commit your initial setup.\n",
        "\n",
        "## 2. Data Preprocessing\n",
        "- [Done] **Load Dataset:**  \n",
        "  Load your dataset into the notebook.\n",
        "  \n",
        "- [Done] **Tokenization:**\n",
        "  - **Character-Level Tokenization:**  \n",
        "    - Tokenize the text data at the character level.\n",
        "    - Save and log the processed data.\n",
        "  - **Word-Level Tokenization:**  \n",
        "    - Modify the tokenization process to tokenize the text by words.\n",
        "    - Save and log the processed data.\n",
        "    \n",
        "- [Done] **Comparison:**  \n",
        "  - Create a section in your notebook to compare the two tokenization approaches.\n",
        "  - Visualize or tabulate differences in vocabulary size, sequence lengths, and other relevant metrics.\n",
        "\n",
        "**Findings from tokenization comparison:**\n",
        "- Bag of Words: Loss 0.9187, Accuracy: 0.8382\n",
        "- Bag of Character: Loss: 0.6614, Accuracy: 0.6064\n",
        "\n",
        "## 3. Model Architecture\n",
        "- [Done] **Define the Model:**  \n",
        "  Develop a model (or models) that can handle both tokenization types. Include the following adjustable hyper-parameters:\n",
        "  - Learning rate\n",
        "  - Number of hidden layers\n",
        "  - Hidden sizes (neurons per layer)\n",
        "  - Batch sizes\n",
        "  - Optimizers (e.g., Adam, SGD, RMSProp)\n",
        "  - Activation functions (e.g., ReLU, Tanh, LeakyReLU)\n",
        "\n",
        "## 4. Hyper-Parameter Optimization\n",
        "- [Done] **Experiment Setup:**  \n",
        "  For each hyper-parameter configuration, perform at least 3 different tests to ensure robustness.\n",
        "  \n",
        "- [Done ] **Grid/Random Search:**  \n",
        "  Set up a search over the following hyper-parameter ranges (example values provided):\n",
        "  - **Learning Rate:** `[0.001, 0.0005, 0.0001]`\n",
        "  - **Hidden Layers:** `[1, 2, 3]`\n",
        "  - **Hidden Sizes:** `[128, 256, 512]`\n",
        "  - **Batch Sizes:** `[32, 64, 128]`\n",
        "  - **Optimizers:** `[Adam, SGD, RMSProp]`\n",
        "  - **Activation Functions:** `[ReLU, Tanh, LeakyReLU]`\n",
        "  \n",
        "- [Done ] **Logging:**  \n",
        "  Record the results (accuracy, loss, etc.) for each configuration in tables or charts.\n",
        "\n",
        "**Best Model:**\n",
        "-\tLearning Rate : 0.0005\n",
        "-\tNumber of Hidden Layers: 2\n",
        "-\tHidden Layer 1 size: 256\n",
        "-\tBatch size: 128\n",
        "-\tOptimizer: Adam\n",
        "-\tActivation function: ReLu\n",
        "\n",
        "Test accuracy: 0.8652\n",
        "\n",
        "## 5. Model Training and Evaluation\n",
        "- [ Done] **Training with Each Configuration:**  \n",
        "  Run experiments for both tokenization approaches with each set of hyper-parameters:\n",
        "  - Train the model at least 3 times per configuration (keeping the seed constant at this stage).\n",
        "  - Log training and validation performance.\n",
        "  \n",
        "- [Done] **Identify the Best Model:**  \n",
        "  Select the best performing configuration based on validation metrics (e.g., accuracy).\n",
        "\n",
        "## 6. Final Experiments\n",
        "- [Done] **Robustness Check:**  \n",
        "  Once the best model is identified:\n",
        "  - Re-run the experiments at least 3 times with different random seeds.\n",
        "  - Record the performance (accuracy) for each run.\n",
        "  \n",
        "- [Done] **Statistical Reporting:**  \n",
        "  - Compute the **mean accuracy** and **standard error** across these runs.\n",
        "  - Include these statistics in your report.\n",
        "\n",
        "## 7. Documentation and Reporting\n",
        "- [Done ] **Jupyter Notebook:**  \n",
        "  - Ensure that your notebook is well-commented and clearly documents each step.\n",
        "  - Include code cells for setting seeds, data preprocessing, model building, training, evaluation, and visualization.\n",
        "  \n",
        "- [Done ] **Detailed Report (Word Document):**  \n",
        "  Prepare a report that includes:\n",
        "  - **Introduction:** Objectives and overview of the work.\n",
        "  - **Methodology:** Detailed explanation of tokenization changes and hyper-parameter optimization strategy.\n",
        "  - **Experiments and Results:**  \n",
        "    - Comparison between character-level and word-level tokenization.\n",
        "    - Tables/graphs for hyper-parameter experiments.\n",
        "    - Final model performance with mean accuracy and standard error.\n",
        "  - **Discussion:** Analysis of results, challenges encountered, and insights.\n",
        "  - **Conclusion:** Summarize the key findings.\n",
        "  \n",
        "- [ ] **Submission:**  \n",
        "  - Submit your Jupyter Notebook.\n",
        "  - Submit your Word document report.\n",
        "  - Ensure that both files are included in your repository or submission package.\n",
        "\n",
        "## 8. Final Checklist\n",
        "- [ ] All experiments have at least 3 different tests.\n",
        "- [ ] Random seeds are set before any experiment.\n",
        "- [ ] Hyper-parameter optimization covers changes in learning rate, hidden layers, hidden sizes, batch sizes, optimizers, and activation functions.\n",
        "- [ ] The best model’s performance is verified with experiments on different seeds.\n",
        "- [ ] Best model should be compared with random model shown above.\n",
        "- [ ] The report clearly documents the methodology, experiments, results, and final conclusions.\n",
        "- [ ] If experiments are shown with deeper MLP_FA with best settings (Extra credits -- 2 points)\n",
        "\n",
        "---\n",
        "\n",
        "> **Note:**  \n",
        "> Keep thorough logs and document any observations during your experiments. Clear documentation is key to reproducibility and understanding your results.\n",
        "\n"
      ],
      "metadata": {
        "id": "6j6mv098aASE"
      }
    }
  ]
}